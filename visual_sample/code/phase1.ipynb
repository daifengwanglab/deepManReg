{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sd\n",
    "from neighborhood import neighbor_graph, laplacian\n",
    "from correspondence import Correspondence\n",
    "from stiefel import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datareader import *\n",
    "import pandas as pd \n",
    "import os.path\n",
    "import pdb\n",
    "cuda = torch.device('cuda') \n",
    "import scipy as sp\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from random import sample\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines the neural network\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1_sigmoid = self.linear1(x).sigmoid()\n",
    "        h2_sigmoid = self.linear2(h1_sigmoid).sigmoid()\n",
    "        y_pred = self.linear3(h2_sigmoid)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_project(x1_np, x2_np):\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H1, H2, D_out = x1_np.shape[0], x1_np.shape[1], 512, 64, 10\n",
    "\n",
    "    model = Net(D_in, H1, H2, D_out)\n",
    "\n",
    "    x1 = torch.from_numpy(x1_np.astype(np.float32))\n",
    "    x2 = torch.from_numpy(x2_np.astype(np.float32))\n",
    "    print(x1.dtype)\n",
    "    \n",
    "    adj1 = neighbor_graph(x1_np, k=5)\n",
    "    adj2 = neighbor_graph(x2_np, k=5)\n",
    "\n",
    "    #corr = Correspondence(matrix=np.eye(N))\n",
    "\n",
    "    w1 = np.corrcoef(x1, x2)[0:x1.shape[0],x1.shape[0]:(x1.shape[0]+x2.shape[0])]\n",
    "    w1[abs(w1) > 0.5] = 1\n",
    "    w1[w1 != 1] = 0\n",
    "    w = np.block([[w1,adj1],\n",
    "                  [adj2,w1.T]])\n",
    "\n",
    "    L_np = laplacian(w, normed=False)\n",
    "    L = torch.from_numpy(L_np.astype(np.float32))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "    \n",
    "    for t in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y1_pred = model(x1)\n",
    "        y2_pred = model(x2)\n",
    "\n",
    "        outputs = torch.cat((y1_pred, y2_pred), 0)\n",
    "        \n",
    "        # Project the output onto Stiefel Manifold\n",
    "        u, s, v = torch.svd(outputs, some=True)\n",
    "        proj_outputs = u@v.t()\n",
    "\n",
    "        # Compute and print loss\n",
    "        print(L.dtype)\n",
    "        loss = torch.trace(proj_outputs.t()@L@proj_outputs)\n",
    "        print(t, loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        proj_outputs.retain_grad()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Project the (Euclidean) gradient onto the tangent space of Stiefel Manifold (to get Rimannian gradient)\n",
    "        rgrad = proj_stiefel(proj_outputs, proj_outputs.grad) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropogate the Rimannian gradient w.r.t proj_outputs\n",
    "        proj_outputs.backward(rgrad)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    proj_outputs_np = proj_outputs.detach().numpy()\n",
    "    return proj_outputs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of geneExp:  (1000, 3654)\n",
      "Shape of Efeature:  (3654, 41)\n",
      "(1000, 3654)\n",
      "(41, 3654)\n"
     ]
    }
   ],
   "source": [
    "Efeature = pd.read_csv('../data/efeature_filtered.csv',index_col=0)\n",
    "geneExp = pd.read_csv('../data/expMat_filtered.csv',index_col=0)\n",
    "label = pd.read_csv('../data/label_visual.csv')\n",
    "print('Shape of geneExp: ', geneExp.shape)\n",
    "print('Shape of Efeature: ', Efeature.shape)\n",
    "\n",
    "#x1_np = preprocessing.scale(np.log(geneExp+1).to_numpy())\n",
    "#x2_np = preprocessing.scale(Efeature.T.to_numpy())\n",
    "\n",
    "x1_np = np.log(geneExp+1).to_numpy()\n",
    "x2_np = preprocessing.scale(Efeature.T.to_numpy())\n",
    "\n",
    "print(x1_np.shape)\n",
    "print(x2_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "0 76.15230560302734\n",
      "torch.float32\n",
      "1 74.04776000976562\n",
      "torch.float32\n",
      "2 72.15640258789062\n",
      "torch.float32\n",
      "3 70.50237274169922\n",
      "torch.float32\n",
      "4 69.06790924072266\n",
      "torch.float32\n",
      "5 67.79653930664062\n",
      "torch.float32\n",
      "6 66.63261413574219\n",
      "torch.float32\n",
      "7 65.54585266113281\n",
      "torch.float32\n",
      "8 64.52932739257812\n",
      "torch.float32\n",
      "9 63.584930419921875\n",
      "torch.float32\n",
      "10 62.70964431762695\n",
      "torch.float32\n",
      "11 61.8917236328125\n",
      "torch.float32\n",
      "12 61.11663818359375\n",
      "torch.float32\n",
      "13 60.37461853027344\n",
      "torch.float32\n",
      "14 59.662174224853516\n",
      "torch.float32\n",
      "15 58.97975158691406\n",
      "torch.float32\n",
      "16 58.32814025878906\n",
      "torch.float32\n",
      "17 57.706607818603516\n",
      "torch.float32\n",
      "18 57.112762451171875\n",
      "torch.float32\n",
      "19 56.5430908203125\n",
      "torch.float32\n",
      "20 55.99395751953125\n",
      "torch.float32\n",
      "21 55.46256637573242\n",
      "torch.float32\n",
      "22 54.947120666503906\n",
      "torch.float32\n",
      "23 54.446781158447266\n",
      "torch.float32\n",
      "24 53.96090316772461\n",
      "torch.float32\n",
      "25 53.4884147644043\n",
      "torch.float32\n",
      "26 53.02803039550781\n",
      "torch.float32\n",
      "27 52.57858657836914\n",
      "torch.float32\n",
      "28 52.13941955566406\n",
      "torch.float32\n",
      "29 51.71017074584961\n",
      "torch.float32\n",
      "30 51.29027557373047\n",
      "torch.float32\n",
      "31 50.87920379638672\n",
      "torch.float32\n",
      "32 50.47650909423828\n",
      "torch.float32\n",
      "33 50.081947326660156\n",
      "torch.float32\n",
      "34 49.695213317871094\n",
      "torch.float32\n",
      "35 49.31554412841797\n",
      "torch.float32\n",
      "36 48.942237854003906\n",
      "torch.float32\n",
      "37 48.57518768310547\n",
      "torch.float32\n",
      "38 48.214744567871094\n",
      "torch.float32\n",
      "39 47.8610725402832\n",
      "torch.float32\n",
      "40 47.51369094848633\n",
      "torch.float32\n",
      "41 47.171783447265625\n",
      "torch.float32\n",
      "42 46.834808349609375\n",
      "torch.float32\n",
      "43 46.50300598144531\n",
      "torch.float32\n",
      "44 46.176544189453125\n",
      "torch.float32\n",
      "45 45.855186462402344\n",
      "torch.float32\n",
      "46 45.53826141357422\n",
      "torch.float32\n",
      "47 45.225372314453125\n",
      "torch.float32\n",
      "48 44.91651153564453\n",
      "torch.float32\n",
      "49 44.61199951171875\n",
      "torch.float32\n",
      "50 44.31186294555664\n",
      "torch.float32\n",
      "51 44.01580047607422\n",
      "torch.float32\n",
      "52 43.72362518310547\n",
      "torch.float32\n",
      "53 43.43531036376953\n",
      "torch.float32\n",
      "54 43.15098571777344\n",
      "torch.float32\n",
      "55 42.87054443359375\n",
      "torch.float32\n",
      "56 42.59373474121094\n",
      "torch.float32\n",
      "57 42.32038497924805\n",
      "torch.float32\n",
      "58 42.05038070678711\n",
      "torch.float32\n",
      "59 41.783756256103516\n",
      "torch.float32\n",
      "60 41.52046203613281\n",
      "torch.float32\n",
      "61 41.26032257080078\n",
      "torch.float32\n",
      "62 41.00327682495117\n",
      "torch.float32\n",
      "63 40.74931335449219\n",
      "torch.float32\n",
      "64 40.498348236083984\n",
      "torch.float32\n",
      "65 40.25020980834961\n",
      "torch.float32\n",
      "66 40.004764556884766\n",
      "torch.float32\n",
      "67 39.76203536987305\n",
      "torch.float32\n",
      "68 39.52197265625\n",
      "torch.float32\n",
      "69 39.28448486328125\n",
      "torch.float32\n",
      "70 39.04950714111328\n",
      "torch.float32\n",
      "71 38.816932678222656\n",
      "torch.float32\n",
      "72 38.58677291870117\n",
      "torch.float32\n",
      "73 38.35895538330078\n",
      "torch.float32\n",
      "74 38.13336944580078\n",
      "torch.float32\n",
      "75 37.909950256347656\n",
      "torch.float32\n",
      "76 37.6887092590332\n",
      "torch.float32\n",
      "77 37.469482421875\n",
      "torch.float32\n",
      "78 37.25230407714844\n",
      "torch.float32\n",
      "79 37.037105560302734\n",
      "torch.float32\n",
      "80 36.823822021484375\n",
      "torch.float32\n",
      "81 36.612430572509766\n",
      "torch.float32\n",
      "82 36.40290069580078\n",
      "torch.float32\n",
      "83 36.19512176513672\n",
      "torch.float32\n",
      "84 35.98907470703125\n",
      "torch.float32\n",
      "85 35.784732818603516\n",
      "torch.float32\n",
      "86 35.58200454711914\n",
      "torch.float32\n",
      "87 35.380859375\n",
      "torch.float32\n",
      "88 35.181312561035156\n",
      "torch.float32\n",
      "89 34.98322296142578\n",
      "torch.float32\n",
      "90 34.78667068481445\n",
      "torch.float32\n",
      "91 34.591548919677734\n",
      "torch.float32\n",
      "92 34.3978271484375\n",
      "torch.float32\n",
      "93 34.20551681518555\n",
      "torch.float32\n",
      "94 34.01454162597656\n",
      "torch.float32\n",
      "95 33.824886322021484\n",
      "torch.float32\n",
      "96 33.63653564453125\n",
      "torch.float32\n",
      "97 33.449432373046875\n",
      "torch.float32\n",
      "98 33.263587951660156\n",
      "torch.float32\n",
      "99 33.07899856567383\n",
      "torch.float32\n",
      "100 32.89558410644531\n",
      "torch.float32\n",
      "101 32.71337890625\n",
      "torch.float32\n",
      "102 32.532325744628906\n",
      "torch.float32\n",
      "103 32.3524284362793\n",
      "torch.float32\n",
      "104 32.173641204833984\n",
      "torch.float32\n",
      "105 31.995975494384766\n",
      "torch.float32\n",
      "106 31.819412231445312\n",
      "torch.float32\n",
      "107 31.643945693969727\n",
      "torch.float32\n",
      "108 31.46955108642578\n",
      "torch.float32\n",
      "109 31.296215057373047\n",
      "torch.float32\n",
      "110 31.12392807006836\n",
      "torch.float32\n",
      "111 30.952699661254883\n",
      "torch.float32\n",
      "112 30.782468795776367\n",
      "torch.float32\n",
      "113 30.61328887939453\n",
      "torch.float32\n",
      "114 30.445117950439453\n",
      "torch.float32\n",
      "115 30.277984619140625\n",
      "torch.float32\n",
      "116 30.111814498901367\n",
      "torch.float32\n",
      "117 29.94667625427246\n",
      "torch.float32\n",
      "118 29.78253173828125\n",
      "torch.float32\n",
      "119 29.61934471130371\n",
      "torch.float32\n",
      "120 29.457191467285156\n",
      "torch.float32\n",
      "121 29.295976638793945\n",
      "torch.float32\n",
      "122 29.13578987121582\n",
      "torch.float32\n",
      "123 28.976531982421875\n",
      "torch.float32\n",
      "124 28.818288803100586\n",
      "torch.float32\n",
      "125 28.661006927490234\n",
      "torch.float32\n",
      "126 28.50469207763672\n",
      "torch.float32\n",
      "127 28.349367141723633\n",
      "torch.float32\n",
      "128 28.19501495361328\n",
      "torch.float32\n",
      "129 28.041627883911133\n",
      "torch.float32\n",
      "130 27.889238357543945\n",
      "torch.float32\n",
      "131 27.73781394958496\n",
      "torch.float32\n",
      "132 27.58738136291504\n",
      "torch.float32\n",
      "133 27.43790054321289\n",
      "torch.float32\n",
      "134 27.289417266845703\n",
      "torch.float32\n",
      "135 27.141931533813477\n",
      "torch.float32\n",
      "136 26.99542999267578\n",
      "torch.float32\n",
      "137 26.84993553161621\n",
      "torch.float32\n",
      "138 26.705419540405273\n",
      "torch.float32\n",
      "139 26.561933517456055\n",
      "torch.float32\n",
      "140 26.4194278717041\n",
      "torch.float32\n",
      "141 26.277931213378906\n",
      "torch.float32\n",
      "142 26.137466430664062\n",
      "torch.float32\n",
      "143 25.998022079467773\n",
      "torch.float32\n",
      "144 25.85958480834961\n",
      "torch.float32\n",
      "145 25.72220230102539\n",
      "torch.float32\n",
      "146 25.585826873779297\n",
      "torch.float32\n",
      "147 25.450515747070312\n",
      "torch.float32\n",
      "148 25.31624412536621\n",
      "torch.float32\n",
      "149 25.183012008666992\n",
      "torch.float32\n",
      "150 25.05084228515625\n",
      "torch.float32\n",
      "151 24.91973876953125\n",
      "torch.float32\n",
      "152 24.789710998535156\n",
      "torch.float32\n",
      "153 24.660736083984375\n",
      "torch.float32\n",
      "154 24.53286361694336\n",
      "torch.float32\n",
      "155 24.40608024597168\n",
      "torch.float32\n",
      "156 24.280366897583008\n",
      "torch.float32\n",
      "157 24.15576934814453\n",
      "torch.float32\n",
      "158 24.03226661682129\n",
      "torch.float32\n",
      "159 23.909873962402344\n",
      "torch.float32\n",
      "160 23.788589477539062\n",
      "torch.float32\n",
      "161 23.668439865112305\n",
      "torch.float32\n",
      "162 23.549396514892578\n",
      "torch.float32\n",
      "163 23.431488037109375\n",
      "torch.float32\n",
      "164 23.314716339111328\n",
      "torch.float32\n",
      "165 23.199066162109375\n",
      "torch.float32\n",
      "166 23.084566116333008\n",
      "torch.float32\n",
      "167 22.97121238708496\n",
      "torch.float32\n",
      "168 22.859012603759766\n",
      "torch.float32\n",
      "169 22.74797248840332\n",
      "torch.float32\n",
      "170 22.6380558013916\n",
      "torch.float32\n",
      "171 22.529308319091797\n",
      "torch.float32\n",
      "172 22.421728134155273\n",
      "torch.float32\n",
      "173 22.315290451049805\n",
      "torch.float32\n",
      "174 22.21002960205078\n",
      "torch.float32\n",
      "175 22.105926513671875\n",
      "torch.float32\n",
      "176 22.002965927124023\n",
      "torch.float32\n",
      "177 21.901180267333984\n",
      "torch.float32\n",
      "178 21.800554275512695\n",
      "torch.float32\n",
      "179 21.701072692871094\n",
      "torch.float32\n",
      "180 21.602773666381836\n",
      "torch.float32\n",
      "181 21.505603790283203\n",
      "torch.float32\n",
      "182 21.409595489501953\n",
      "torch.float32\n",
      "183 21.314746856689453\n",
      "torch.float32\n",
      "184 21.221038818359375\n",
      "torch.float32\n",
      "185 21.128459930419922\n",
      "torch.float32\n",
      "186 21.037029266357422\n",
      "torch.float32\n",
      "187 20.94673728942871\n",
      "torch.float32\n",
      "188 20.85756492614746\n",
      "torch.float32\n",
      "189 20.76951789855957\n",
      "torch.float32\n",
      "190 20.68259048461914\n",
      "torch.float32\n",
      "191 20.59676170349121\n",
      "torch.float32\n",
      "192 20.51205062866211\n",
      "torch.float32\n",
      "193 20.428424835205078\n",
      "torch.float32\n",
      "194 20.345901489257812\n",
      "torch.float32\n",
      "195 20.264455795288086\n",
      "torch.float32\n",
      "196 20.18409538269043\n",
      "torch.float32\n",
      "197 20.104782104492188\n",
      "torch.float32\n",
      "198 20.026538848876953\n",
      "torch.float32\n",
      "199 19.9493408203125\n",
      "torch.float32\n",
      "200 19.87318992614746\n",
      "torch.float32\n",
      "201 19.798063278198242\n",
      "torch.float32\n",
      "202 19.723970413208008\n",
      "torch.float32\n",
      "203 19.650890350341797\n",
      "torch.float32\n",
      "204 19.578805923461914\n",
      "torch.float32\n",
      "205 19.507719039916992\n",
      "torch.float32\n",
      "206 19.43762969970703\n",
      "torch.float32\n",
      "207 19.36850929260254\n",
      "torch.float32\n",
      "208 19.300342559814453\n",
      "torch.float32\n",
      "209 19.23314094543457\n",
      "torch.float32\n",
      "210 19.16687774658203\n",
      "torch.float32\n",
      "211 19.101564407348633\n",
      "torch.float32\n",
      "212 19.037158966064453\n",
      "torch.float32\n",
      "213 18.973674774169922\n",
      "torch.float32\n",
      "214 18.911090850830078\n",
      "torch.float32\n",
      "215 18.849414825439453\n",
      "torch.float32\n",
      "216 18.788599014282227\n",
      "torch.float32\n",
      "217 18.728670120239258\n",
      "torch.float32\n",
      "218 18.66960334777832\n",
      "torch.float32\n",
      "219 18.61138916015625\n",
      "torch.float32\n",
      "220 18.55400848388672\n",
      "torch.float32\n",
      "221 18.49746322631836\n",
      "torch.float32\n",
      "222 18.44173240661621\n",
      "torch.float32\n",
      "223 18.386808395385742\n",
      "torch.float32\n",
      "224 18.332691192626953\n",
      "torch.float32\n",
      "225 18.27935218811035\n",
      "torch.float32\n",
      "226 18.2268009185791\n",
      "torch.float32\n",
      "227 18.175012588500977\n",
      "torch.float32\n",
      "228 18.12398338317871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "229 18.07370376586914\n",
      "torch.float32\n",
      "230 18.024158477783203\n",
      "torch.float32\n",
      "231 17.975345611572266\n",
      "torch.float32\n",
      "232 17.927244186401367\n",
      "torch.float32\n",
      "233 17.87984848022461\n",
      "torch.float32\n",
      "234 17.833160400390625\n",
      "torch.float32\n",
      "235 17.787141799926758\n",
      "torch.float32\n",
      "236 17.741823196411133\n",
      "torch.float32\n",
      "237 17.697160720825195\n",
      "torch.float32\n",
      "238 17.653154373168945\n",
      "torch.float32\n",
      "239 17.609806060791016\n",
      "torch.float32\n",
      "240 17.567089080810547\n",
      "torch.float32\n",
      "241 17.525026321411133\n",
      "torch.float32\n",
      "242 17.483558654785156\n",
      "torch.float32\n",
      "243 17.442720413208008\n",
      "torch.float32\n",
      "244 17.402477264404297\n",
      "torch.float32\n",
      "245 17.36283302307129\n",
      "torch.float32\n",
      "246 17.323768615722656\n",
      "torch.float32\n",
      "247 17.285293579101562\n",
      "torch.float32\n",
      "248 17.24738883972168\n",
      "torch.float32\n",
      "249 17.210023880004883\n",
      "torch.float32\n",
      "250 17.17322540283203\n",
      "torch.float32\n",
      "251 17.1369686126709\n",
      "torch.float32\n",
      "252 17.101255416870117\n",
      "torch.float32\n",
      "253 17.066070556640625\n",
      "torch.float32\n",
      "254 17.031389236450195\n",
      "torch.float32\n",
      "255 16.99723243713379\n",
      "torch.float32\n",
      "256 16.96358871459961\n",
      "torch.float32\n",
      "257 16.930429458618164\n",
      "torch.float32\n",
      "258 16.89775848388672\n",
      "torch.float32\n",
      "259 16.865568161010742\n",
      "torch.float32\n",
      "260 16.833845138549805\n",
      "torch.float32\n",
      "261 16.802610397338867\n",
      "torch.float32\n",
      "262 16.771831512451172\n",
      "torch.float32\n",
      "263 16.741498947143555\n",
      "torch.float32\n",
      "264 16.71160125732422\n",
      "torch.float32\n",
      "265 16.68215560913086\n",
      "torch.float32\n",
      "266 16.653152465820312\n",
      "torch.float32\n",
      "267 16.624549865722656\n",
      "torch.float32\n",
      "268 16.596372604370117\n",
      "torch.float32\n",
      "269 16.56861686706543\n",
      "torch.float32\n",
      "270 16.541271209716797\n",
      "torch.float32\n",
      "271 16.514310836791992\n",
      "torch.float32\n",
      "272 16.487749099731445\n",
      "torch.float32\n",
      "273 16.461585998535156\n",
      "torch.float32\n",
      "274 16.4357852935791\n",
      "torch.float32\n",
      "275 16.410377502441406\n",
      "torch.float32\n",
      "276 16.385330200195312\n",
      "torch.float32\n",
      "277 16.360639572143555\n",
      "torch.float32\n",
      "278 16.33631134033203\n",
      "torch.float32\n",
      "279 16.31233787536621\n",
      "torch.float32\n",
      "280 16.288715362548828\n",
      "torch.float32\n",
      "281 16.265419006347656\n",
      "torch.float32\n",
      "282 16.242469787597656\n",
      "torch.float32\n",
      "283 16.2198486328125\n",
      "torch.float32\n",
      "284 16.19755744934082\n",
      "torch.float32\n",
      "285 16.175579071044922\n",
      "torch.float32\n",
      "286 16.1539249420166\n",
      "torch.float32\n",
      "287 16.132568359375\n",
      "torch.float32\n",
      "288 16.111515045166016\n",
      "torch.float32\n",
      "289 16.090768814086914\n",
      "torch.float32\n",
      "290 16.0703182220459\n",
      "torch.float32\n",
      "291 16.050161361694336\n",
      "torch.float32\n",
      "292 16.03028106689453\n",
      "torch.float32\n",
      "293 16.010683059692383\n",
      "torch.float32\n",
      "294 15.991372108459473\n",
      "torch.float32\n",
      "295 15.972326278686523\n",
      "torch.float32\n",
      "296 15.953545570373535\n",
      "torch.float32\n",
      "297 15.935042381286621\n",
      "torch.float32\n",
      "298 15.916784286499023\n",
      "torch.float32\n",
      "299 15.898792266845703\n",
      "torch.float32\n",
      "300 15.881033897399902\n",
      "torch.float32\n",
      "301 15.863533973693848\n",
      "torch.float32\n",
      "302 15.846280097961426\n",
      "torch.float32\n",
      "303 15.829273223876953\n",
      "torch.float32\n",
      "304 15.812482833862305\n",
      "torch.float32\n",
      "305 15.795940399169922\n",
      "torch.float32\n",
      "306 15.779610633850098\n",
      "torch.float32\n",
      "307 15.763521194458008\n",
      "torch.float32\n",
      "308 15.74764347076416\n",
      "torch.float32\n",
      "309 15.731980323791504\n",
      "torch.float32\n",
      "310 15.716535568237305\n",
      "torch.float32\n",
      "311 15.701298713684082\n",
      "torch.float32\n",
      "312 15.68626880645752\n",
      "torch.float32\n",
      "313 15.6714448928833\n",
      "torch.float32\n",
      "314 15.656822204589844\n",
      "torch.float32\n",
      "315 15.642388343811035\n",
      "torch.float32\n",
      "316 15.628158569335938\n",
      "torch.float32\n",
      "317 15.61411190032959\n",
      "torch.float32\n",
      "318 15.600247383117676\n",
      "torch.float32\n",
      "319 15.586584091186523\n",
      "torch.float32\n",
      "320 15.573088645935059\n",
      "torch.float32\n",
      "321 15.559795379638672\n",
      "torch.float32\n",
      "322 15.54665470123291\n",
      "torch.float32\n",
      "323 15.533682823181152\n",
      "torch.float32\n",
      "324 15.520894050598145\n",
      "torch.float32\n",
      "325 15.508273124694824\n",
      "torch.float32\n",
      "326 15.495809555053711\n",
      "torch.float32\n",
      "327 15.483508110046387\n",
      "torch.float32\n",
      "328 15.4713716506958\n",
      "torch.float32\n",
      "329 15.459402084350586\n",
      "torch.float32\n",
      "330 15.447558403015137\n",
      "torch.float32\n",
      "331 15.435882568359375\n",
      "torch.float32\n",
      "332 15.424355506896973\n",
      "torch.float32\n",
      "333 15.412986755371094\n",
      "torch.float32\n",
      "334 15.401747703552246\n",
      "torch.float32\n",
      "335 15.390652656555176\n",
      "torch.float32\n",
      "336 15.379698753356934\n",
      "torch.float32\n",
      "337 15.368881225585938\n",
      "torch.float32\n",
      "338 15.358211517333984\n",
      "torch.float32\n",
      "339 15.347661018371582\n",
      "torch.float32\n",
      "340 15.33724594116211\n",
      "torch.float32\n",
      "341 15.326964378356934\n",
      "torch.float32\n",
      "342 15.316804885864258\n",
      "torch.float32\n",
      "343 15.306768417358398\n",
      "torch.float32\n",
      "344 15.296859741210938\n",
      "torch.float32\n",
      "345 15.287067413330078\n",
      "torch.float32\n",
      "346 15.277399063110352\n",
      "torch.float32\n",
      "347 15.26783561706543\n",
      "torch.float32\n",
      "348 15.258407592773438\n",
      "torch.float32\n",
      "349 15.2490816116333\n",
      "torch.float32\n",
      "350 15.239862442016602\n",
      "torch.float32\n",
      "351 15.23076057434082\n",
      "torch.float32\n",
      "352 15.221763610839844\n",
      "torch.float32\n",
      "353 15.212875366210938\n",
      "torch.float32\n",
      "354 15.204081535339355\n",
      "torch.float32\n",
      "355 15.195408821105957\n",
      "torch.float32\n",
      "356 15.186822891235352\n",
      "torch.float32\n",
      "357 15.178351402282715\n",
      "torch.float32\n",
      "358 15.169957160949707\n",
      "torch.float32\n",
      "359 15.161674499511719\n",
      "torch.float32\n",
      "360 15.153473854064941\n",
      "torch.float32\n",
      "361 15.145380020141602\n",
      "torch.float32\n",
      "362 15.137372016906738\n",
      "torch.float32\n",
      "363 15.129462242126465\n",
      "torch.float32\n",
      "364 15.12163257598877\n",
      "torch.float32\n",
      "365 15.113895416259766\n",
      "torch.float32\n",
      "366 15.106240272521973\n",
      "torch.float32\n",
      "367 15.098676681518555\n",
      "torch.float32\n",
      "368 15.091184616088867\n",
      "torch.float32\n",
      "369 15.083776473999023\n",
      "torch.float32\n",
      "370 15.076462745666504\n",
      "torch.float32\n",
      "371 15.069219589233398\n",
      "torch.float32\n",
      "372 15.062056541442871\n",
      "torch.float32\n",
      "373 15.054970741271973\n",
      "torch.float32\n",
      "374 15.047962188720703\n",
      "torch.float32\n",
      "375 15.041031837463379\n",
      "torch.float32\n",
      "376 15.034160614013672\n",
      "torch.float32\n",
      "377 15.027372360229492\n",
      "torch.float32\n",
      "378 15.020659446716309\n",
      "torch.float32\n",
      "379 15.014010429382324\n",
      "torch.float32\n",
      "380 15.007442474365234\n",
      "torch.float32\n",
      "381 15.00092887878418\n",
      "torch.float32\n",
      "382 14.994485855102539\n",
      "torch.float32\n",
      "383 14.988105773925781\n",
      "torch.float32\n",
      "384 14.981799125671387\n",
      "torch.float32\n",
      "385 14.97554874420166\n",
      "torch.float32\n",
      "386 14.96937084197998\n",
      "torch.float32\n",
      "387 14.963250160217285\n",
      "torch.float32\n",
      "388 14.95719051361084\n",
      "torch.float32\n",
      "389 14.951190948486328\n",
      "torch.float32\n",
      "390 14.945260047912598\n",
      "torch.float32\n",
      "391 14.939377784729004\n",
      "torch.float32\n",
      "392 14.933554649353027\n",
      "torch.float32\n",
      "393 14.927788734436035\n",
      "torch.float32\n",
      "394 14.922087669372559\n",
      "torch.float32\n",
      "395 14.916422843933105\n",
      "torch.float32\n",
      "396 14.910823822021484\n",
      "torch.float32\n",
      "397 14.905281066894531\n",
      "torch.float32\n",
      "398 14.899789810180664\n",
      "torch.float32\n",
      "399 14.894353866577148\n",
      "torch.float32\n",
      "400 14.888961791992188\n",
      "torch.float32\n",
      "401 14.883625984191895\n",
      "torch.float32\n",
      "402 14.878332138061523\n",
      "torch.float32\n",
      "403 14.873098373413086\n",
      "torch.float32\n",
      "404 14.867901802062988\n",
      "torch.float32\n",
      "405 14.862765312194824\n",
      "torch.float32\n",
      "406 14.857672691345215\n",
      "torch.float32\n",
      "407 14.852615356445312\n",
      "torch.float32\n",
      "408 14.847622871398926\n",
      "torch.float32\n",
      "409 14.842667579650879\n",
      "torch.float32\n",
      "410 14.83775520324707\n",
      "torch.float32\n",
      "411 14.832881927490234\n",
      "torch.float32\n",
      "412 14.828062057495117\n",
      "torch.float32\n",
      "413 14.823286056518555\n",
      "torch.float32\n",
      "414 14.818538665771484\n",
      "torch.float32\n",
      "415 14.813865661621094\n",
      "torch.float32\n",
      "416 14.809226989746094\n",
      "torch.float32\n",
      "417 14.804671287536621\n",
      "torch.float32\n",
      "418 14.80019760131836\n",
      "torch.float32\n",
      "419 14.795905113220215\n",
      "torch.float32\n",
      "420 14.791925430297852\n",
      "torch.float32\n",
      "421 14.788582801818848\n",
      "torch.float32\n",
      "422 14.786483764648438\n",
      "torch.float32\n",
      "423 14.786717414855957\n",
      "torch.float32\n",
      "424 14.789582252502441\n",
      "torch.float32\n",
      "425 14.793619155883789\n",
      "torch.float32\n",
      "426 14.789894104003906\n",
      "torch.float32\n",
      "427 14.775667190551758\n",
      "torch.float32\n",
      "428 14.759153366088867\n",
      "torch.float32\n",
      "429 14.7542142868042\n",
      "torch.float32\n",
      "430 14.758576393127441\n",
      "torch.float32\n",
      "431 14.758342742919922\n",
      "torch.float32\n",
      "432 14.748188972473145\n",
      "torch.float32\n",
      "433 14.7376070022583\n",
      "torch.float32\n",
      "434 14.735986709594727\n",
      "torch.float32\n",
      "435 14.73754596710205\n",
      "torch.float32\n",
      "436 14.73253345489502\n",
      "torch.float32\n",
      "437 14.72352409362793\n",
      "torch.float32\n",
      "438 14.71910285949707\n",
      "torch.float32\n",
      "439 14.719047546386719\n",
      "torch.float32\n",
      "440 14.716262817382812\n",
      "torch.float32\n",
      "441 14.709335327148438\n",
      "torch.float32\n",
      "442 14.704219818115234\n",
      "torch.float32\n",
      "443 14.702740669250488\n",
      "torch.float32\n",
      "444 14.700477600097656\n",
      "torch.float32\n",
      "445 14.69522762298584\n",
      "torch.float32\n",
      "446 14.69022274017334\n",
      "torch.float32\n",
      "447 14.68776798248291\n",
      "torch.float32\n",
      "448 14.685578346252441\n",
      "torch.float32\n",
      "449 14.681382179260254\n",
      "torch.float32\n",
      "450 14.676740646362305\n",
      "torch.float32\n",
      "451 14.673698425292969\n",
      "torch.float32\n",
      "452 14.671351432800293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "453 14.66789436340332\n",
      "torch.float32\n",
      "454 14.663707733154297\n",
      "torch.float32\n",
      "455 14.660327911376953\n",
      "torch.float32\n",
      "456 14.657746315002441\n",
      "torch.float32\n",
      "457 14.654730796813965\n",
      "torch.float32\n",
      "458 14.651023864746094\n",
      "torch.float32\n",
      "459 14.647530555725098\n",
      "torch.float32\n",
      "460 14.644716262817383\n",
      "torch.float32\n",
      "461 14.6419095993042\n",
      "torch.float32\n",
      "462 14.638609886169434\n",
      "torch.float32\n",
      "463 14.635212898254395\n",
      "torch.float32\n",
      "464 14.632189750671387\n",
      "torch.float32\n",
      "465 14.629429817199707\n",
      "torch.float32\n",
      "466 14.626461029052734\n",
      "torch.float32\n",
      "467 14.623247146606445\n",
      "torch.float32\n",
      "468 14.620165824890137\n",
      "torch.float32\n",
      "469 14.617340087890625\n",
      "torch.float32\n",
      "470 14.614527702331543\n",
      "torch.float32\n",
      "471 14.611571311950684\n",
      "torch.float32\n",
      "472 14.60853099822998\n",
      "torch.float32\n",
      "473 14.605644226074219\n",
      "torch.float32\n",
      "474 14.602882385253906\n",
      "torch.float32\n",
      "475 14.600107192993164\n",
      "torch.float32\n",
      "476 14.597232818603516\n",
      "torch.float32\n",
      "477 14.594350814819336\n",
      "torch.float32\n",
      "478 14.591574668884277\n",
      "torch.float32\n",
      "479 14.588873863220215\n",
      "torch.float32\n",
      "480 14.586136817932129\n",
      "torch.float32\n",
      "481 14.583370208740234\n",
      "torch.float32\n",
      "482 14.580609321594238\n",
      "torch.float32\n",
      "483 14.577919006347656\n",
      "torch.float32\n",
      "484 14.575271606445312\n",
      "torch.float32\n",
      "485 14.572623252868652\n",
      "torch.float32\n",
      "486 14.5699462890625\n",
      "torch.float32\n",
      "487 14.567289352416992\n",
      "torch.float32\n",
      "488 14.564664840698242\n",
      "torch.float32\n",
      "489 14.5620756149292\n",
      "torch.float32\n",
      "490 14.559505462646484\n",
      "torch.float32\n",
      "491 14.556924819946289\n",
      "torch.float32\n",
      "492 14.554347038269043\n",
      "torch.float32\n",
      "493 14.551803588867188\n",
      "torch.float32\n",
      "494 14.549278259277344\n",
      "torch.float32\n",
      "495 14.546786308288574\n",
      "torch.float32\n",
      "496 14.544271469116211\n",
      "torch.float32\n",
      "497 14.54178524017334\n",
      "torch.float32\n",
      "498 14.539308547973633\n",
      "torch.float32\n",
      "499 14.536837577819824\n"
     ]
    }
   ],
   "source": [
    "projections = train_and_project(x1_np, x2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1041, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adarb2</th>\n",
       "      <td>-0.006427</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>-0.024288</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.020238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sst</th>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.010013</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>-0.011400</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>-0.022721</td>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.009220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vip</th>\n",
       "      <td>-0.010553</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>-0.026217</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.014434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Npy</th>\n",
       "      <td>-0.003612</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>0.026711</td>\n",
       "      <td>-0.004672</td>\n",
       "      <td>-0.005764</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>-0.021566</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.007751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Synpr</th>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.026262</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.029322</td>\n",
       "      <td>-0.009616</td>\n",
       "      <td>-0.021698</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>-0.030245</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>-0.002280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fast_trough_v_short_square</th>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>-0.005012</td>\n",
       "      <td>0.009959</td>\n",
       "      <td>-0.005279</td>\n",
       "      <td>-0.011582</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.017237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fast_trough_t_short_square</th>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.017510</td>\n",
       "      <td>-0.004141</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>-0.015877</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>0.017557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold_v_short_square</th>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>-0.004602</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>-0.005785</td>\n",
       "      <td>-0.011797</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.018193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold_i_short_square</th>\n",
       "      <td>-0.009573</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.010290</td>\n",
       "      <td>0.022886</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>0.015415</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>-0.024236</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.005419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold_t_short_square</th>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.017508</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.015877</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>0.017559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1041 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2         3         4  \\\n",
       "Adarb2                     -0.006427  0.003653  0.011276  0.008796 -0.002035   \n",
       "Sst                         0.003257  0.010013  0.011801  0.022704 -0.011400   \n",
       "Vip                        -0.010553  0.004490  0.009969  0.012553 -0.004974   \n",
       "Npy                        -0.003612  0.005848 -0.000513  0.026711 -0.004672   \n",
       "Synpr                       0.004285  0.026262  0.018568  0.029322 -0.009616   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "fast_trough_v_short_square  0.002473  0.003269  0.002758  0.019747 -0.005012   \n",
       "fast_trough_t_short_square  0.003247  0.004186  0.000356  0.017510 -0.004141   \n",
       "threshold_v_short_square    0.001886  0.002282 -0.000514  0.018300 -0.004602   \n",
       "threshold_i_short_square   -0.009573  0.020358  0.010290  0.022886 -0.001082   \n",
       "threshold_t_short_square    0.003246  0.004184  0.000351  0.017508 -0.004142   \n",
       "\n",
       "                                   5         6         7         8         9  \n",
       "Adarb2                      0.010519  0.011852 -0.024288  0.004697  0.020238  \n",
       "Sst                        -0.004367  0.009136 -0.022721  0.004323  0.009220  \n",
       "Vip                         0.007019  0.003081 -0.026217  0.006336  0.014434  \n",
       "Npy                        -0.005764  0.004483 -0.021566  0.004539  0.007751  \n",
       "Synpr                      -0.021698  0.002789 -0.030245  0.000830 -0.002280  \n",
       "...                              ...       ...       ...       ...       ...  \n",
       "fast_trough_v_short_square  0.009959 -0.005279 -0.011582  0.001191  0.017237  \n",
       "fast_trough_t_short_square  0.009430 -0.003541 -0.015877 -0.001935  0.017557  \n",
       "threshold_v_short_square    0.011816 -0.005785 -0.011797  0.001866  0.018193  \n",
       "threshold_i_short_square    0.015415  0.000827 -0.024236 -0.000262  0.005419  \n",
       "threshold_t_short_square    0.009429 -0.003540 -0.015877 -0.001934  0.017559  \n",
       "\n",
       "[1041 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projections = pd.DataFrame(projections)\n",
    "features = geneExp.index.tolist()+Efeature.columns.tolist()\n",
    "projections.index = features\n",
    "projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections.to_csv(\"../data/deepmanreg_latent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
